{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\vjk20\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n",
      "C:\\Users\\vjk20\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:30: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\vjk20\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:167: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\vjk20\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:284: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n",
      "C:\\Users\\vjk20\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\Users\\vjk20\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1101: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\Users\\vjk20\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1127: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "C:\\Users\\vjk20\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1362: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\vjk20\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1602: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\vjk20\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1738: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
      "C:\\Users\\vjk20\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:29: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  EPS = np.finfo(np.float).eps\n",
      "C:\\Users\\vjk20\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py:32: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  from ._gradient_boosting import predict_stages\n",
      "C:\\Users\\vjk20\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py:32: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  from ._gradient_boosting import predict_stages\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import sklearn.feature_extraction.text as skft\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sklearn.metrics as skmetrics\n",
    "import sklearn.pipeline as skpipe\n",
    "import sklearn.decomposition as skd\n",
    "import sklearn.naive_bayes as sknb\n",
    "from nltk.corpus import stopwords\n",
    "import wordcloud\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset and create separate dataframe for phishing and non phishing emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_phish_df(my_dir): #dataframe for phishing emails with columns as title.content and label as 1(phish)\n",
    "    titles = []\n",
    "    contents = []\n",
    "    labels = []\n",
    "\n",
    "    for f in os.listdir(os.path.join('',my_dir)):\n",
    "            with open(os.path.join('', my_dir, f), 'r') as reader:\n",
    "                try:\n",
    "                    c = reader.read()\n",
    "                except:\n",
    "                    continue\n",
    "                contents.append(c)\n",
    "                titles.append(f)\n",
    "                labels.append(1)\n",
    "\n",
    "    df = pd.DataFrame({'title': titles, 'content': contents, 'label': 1},\n",
    "                        columns = ['label', 'title', 'content'])\n",
    "    return df\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "phish_email_list = [r\"../Phishing-Detection/phish/20051114\", \n",
    "r\"../Phishing-Detection/phish/phishing0\", \n",
    "r\"../Phishing-Detection/phish/phishing1\", \n",
    "r\"../Phishing-Detection/phish/phishing2\", \n",
    "r\"../Phishing-Detection/phish/phishing3\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "phish_lst = []\n",
    "for phish_folder in phish_email_list:\n",
    "    phish_lst.append(create_phish_df(phish_folder))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phish = pd.concat(phish_lst)\n",
    "df_phish = df_phish[:5000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_email_list = [r\"../Phishing-Detection/enron3\", r\"../Phishing-Detection/enron4\", r\"../Phishing-Detection/enron5\",\n",
    " r\"../Phishing-Detection/enron6\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ham_df(my_dir): #create dataframe for non phishing email set(ham means non phish) and label as 0.\n",
    "                                      #The other columns are content and title\n",
    "    titles = []\n",
    "    contents = []\n",
    "    labels = []\n",
    "\n",
    "    for f in os.listdir(os.path.join(my_dir,'ham')):\n",
    "            with open(os.path.join(my_dir, 'ham', f), 'r') as reader:\n",
    "                try:\n",
    "                    c = reader.read()\n",
    "                except:\n",
    "                    continue\n",
    "                contents.append(c)\n",
    "                titles.append(f)\n",
    "                labels.append('0')\n",
    "\n",
    "    df = pd.DataFrame({'title': titles, 'content': contents, 'label': 0},\n",
    "                        columns = ['label', 'title', 'content'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_list = []\n",
    "for ham in ham_email_list:\n",
    "    ham_list.append(create_ham_df(ham))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   label                            title  \\\n",
       "0      0  0001.2001-02-07.kitchen.ham.txt   \n",
       "1      0  0003.2001-02-08.kitchen.ham.txt   \n",
       "2      0  0005.2001-02-08.kitchen.ham.txt   \n",
       "3      0  0006.2001-02-08.kitchen.ham.txt   \n",
       "4      0  0007.2001-02-09.kitchen.ham.txt   \n",
       "\n",
       "                                             content  \n",
       "0  Subject: key hr issues going forward\\na ) year...  \n",
       "1  Subject: re : key hr issues going forward\\nall...  \n",
       "2  Subject: epmi files protest of entergy transco...  \n",
       "3  Subject: california power 2 / 8\\nplease contac...  \n",
       "4  Subject: california power 2 / 9\\nthe following...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>title</th>\n      <th>content</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0001.2001-02-07.kitchen.ham.txt</td>\n      <td>Subject: key hr issues going forward\\na ) year...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0003.2001-02-08.kitchen.ham.txt</td>\n      <td>Subject: re : key hr issues going forward\\nall...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0005.2001-02-08.kitchen.ham.txt</td>\n      <td>Subject: epmi files protest of entergy transco...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0006.2001-02-08.kitchen.ham.txt</td>\n      <td>Subject: california power 2 / 8\\nplease contac...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0007.2001-02-09.kitchen.ham.txt</td>\n      <td>Subject: california power 2 / 9\\nthe following...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "df_ham = pd.concat(ham_list)\n",
    "df_ham = df_ham[:5000]\n",
    "df_ham.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emails = pd.concat([df_ham, df_phish])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emails_train, df_emails_test = train_test_split(df_emails, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " First text data is cleaned, by tokenising and lemmatising the text using wordnet, removing stop words,removing non-alphabetic strings. Bag-of-words approach (BOW) is used.  We look at the histogram of the words within the text, i.e. considering each word count as a feature.The intuition is that documents are similar if they have similar content. Further, that from the content alone we can learn something about the meaning of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vjk20\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "   label                               title  \\\n",
      "0      0  0002.2001-05-25.SA_and_HP.spam.txt   \n",
      "1      0  0004.2001-06-12.SA_and_HP.spam.txt   \n",
      "2      0  0005.2001-06-23.SA_and_HP.spam.txt   \n",
      "3      0  0006.2001-06-25.SA_and_HP.spam.txt   \n",
      "4      0  0008.2001-06-25.SA_and_HP.spam.txt   \n",
      "\n",
      "                                             content  predicted_label  \n",
      "0  Subject: fw : this is the solution i mentioned...                1  \n",
      "1  Subject: spend too much on your phone bill ? 2...                0  \n",
      "2  Subject: discounted mortgage broker 512517\\nmo...                0  \n",
      "3  Subject: looking 4 real fun 211075433222\\ntalk...                0  \n",
      "4  Subject: your membership exchange , issue # 42...                0  \n"
     ]
    }
   ],
   "source": [
    "#frequency distribution\n",
    "nltk.download('stopwords')\n",
    "text_all = '\\n'.join(df_emails_train.content).lower()\n",
    "stop_words = set(stopwords.words('english')) #remove stopwords like a,an,for,the etc from the text\n",
    "tokenizer = RegexpTokenizer(r'\\w+')#nltk.tokenize.wordpunct_tokenize(text_all)\n",
    "tokens_all = tokenizer.tokenize(text_all)\n",
    "tokens_all = [word for word in tokens_all if word not in stop_words and word != 'font' and word != 'subject']#word not in string.punctuation\n",
    "\n",
    "fd = nltk.probability.FreqDist(tokens_all)\n",
    "\n",
    "phish_text_all = '\\n'.join(df_phish.content).lower()\n",
    "phish_tokens_all = tokenizer.tokenize(phish_text_all)\n",
    "phish_tokens_all = [word for word in phish_tokens_all if word not in stop_words and word != 'font' and word != 'subject']\n",
    "\n",
    "fd_phish = nltk.probability.FreqDist(phish_tokens_all)\n",
    "\n",
    "pipeline = skpipe.Pipeline(\n",
    "    steps = [('vect', skft.CountVectorizer(max_df=0.7)), #convert to numerical feature vectors\n",
    "    ('tfidf', skft.TfidfTransformer()), #term frequency ,inverse document frequency\n",
    "    ('clf', sknb.MultinomialNB())]) #multinomial naive bayes classification\n",
    "\n",
    "df_emails_train, df_emails_test = train_test_split(df_emails, test_size=0.3, random_state=0)\n",
    "pipeline.fit(df_emails_train.content, df_emails_train.label)\n",
    "\n",
    "nb_test_predicted = pipeline.predict(df_emails_test.content)\n",
    "\n",
    "titles = []\n",
    "contents = []\n",
    "labels = []\n",
    "\n",
    "for f in os.listdir(os.path.join(r\"../Phishing-Detection/enron5\",'spam')):\n",
    "        with open(os.path.join(r\"../Phishing-Detection/enron5\", 'spam', f), 'r') as reader:\n",
    "            try:\n",
    "                c = reader.read()\n",
    "            except:\n",
    "                continue\n",
    "            contents.append(c)\n",
    "            titles.append(f)\n",
    "            labels.append(0)\n",
    "\n",
    "df_spam = pd.DataFrame({'title': titles, 'content': contents, 'label': 0},\n",
    "                    columns = ['label', 'title', 'content'])\n",
    "\n",
    "predictions = pipeline.predict(df_spam.content)\n",
    "\n",
    "df_spam['predicted_label'] = predictions\n",
    "\n",
    "print(df_spam[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3675\n",
      "3675\n",
      "[[2700  975]\n",
      " [   0    0]]\n",
      "Accuracy Score : 0.7346938775510204\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.73      0.85      3675\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.73      3675\n",
      "   macro avg       0.50      0.37      0.42      3675\n",
      "weighted avg       1.00      0.73      0.85      3675\n",
      "\n",
      "C:\\Users\\vjk20\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\classification.py:1438: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  recall = _prf_divide(tp_sum, true_sum,\n"
     ]
    }
   ],
   "source": [
    "x= list(df_spam.label)\n",
    "y= list(predictions)\n",
    "print(len(x))\n",
    "print(len(y))\n",
    "results = confusion_matrix(x,y) \n",
    "#print('Confusion Matrix :')\n",
    "print(results) \n",
    "print('Accuracy Score :',accuracy_score(x,y))\n",
    "#print ('Report : ')\n",
    "print(classification_report(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[1482    3]\n [ 114 1395]]\nAccuracy Score : 0.9609218436873748\n              precision    recall  f1-score   support\n\n           0       0.93      1.00      0.96      1485\n           1       1.00      0.92      0.96      1509\n\n    accuracy                           0.96      2994\n   macro avg       0.96      0.96      0.96      2994\nweighted avg       0.96      0.96      0.96      2994\n\n"
     ]
    }
   ],
   "source": [
    "predictions1=list(nb_test_predicted)\n",
    "test_label=list(df_emails_test.label)\n",
    "results = confusion_matrix(test_label, predictions1) \n",
    "#print('Confusion Matrix :')\n",
    "print(results) \n",
    "print('Accuracy Score :',accuracy_score(test_label,predictions1))\n",
    "#print ('Report : ')\n",
    "print(classification_report(test_label, predictions1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'finalized1_text_model.sav'\n",
    "pickle.dump(pipeline, open(filename, 'wb'))\n",
    " \n",
    "# some time later...\n",
    " \n",
    "# load the model from disk\n",
    "#loaded_model = pickle.load(open(filename, 'rb'))\n",
    "#result = loaded_model.score(X_test, Y_test)\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python392jvsc74a57bd0ffdf88481fd2803f97a7684257c56852e7f48b979b63ab0bf002b9c6de0ab9e8",
   "display_name": "Python 3.9.2 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2-final"
  },
  "metadata": {
   "interpreter": {
    "hash": "ffdf88481fd2803f97a7684257c56852e7f48b979b63ab0bf002b9c6de0ab9e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}